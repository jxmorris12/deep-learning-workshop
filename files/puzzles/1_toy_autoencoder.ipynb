{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Puzzle #1\n",
        "## Practical Deep Learning Workshop, Cornell Tech\n",
        "## January 22nd, 2024\n",
        "\n",
        "This notebook contains boilerplate Python code for initializing a multilayer perceptron and training it to remove noise from a batch of data. Below the code that defines the `nn.Module` is some very minimal optimization code. We initialize the neural network and train it to minimize the reconstruction error between `x` and `y`. Since the training set is very small (10 inputs, each a vector of length 5) even this small neural network should be able to perfectly fit the data. This means that the loss should be close to zero.\n",
        "\n",
        "However, something is wrong– if you run the code, you'll notice the loss stagnates; it won't drop below `0.3` or so. This indicates to us that there must be an error somewhere. This pattern of \"failing silently\" is very common in deep learning systems code. There isn't a compilation or runtime error; in fact, the code *looks right*, at least at first glance, but something is clearly amiss.\n",
        "\n",
        "So what's gone wrong? Maybe it's an optimization bug, or a problem with the data, or problem with our nn.Module. This is the tricky part about debugging these systems: we have to understand all the components and how they interact so that we might locate the one or two lines of code that aren't quite right. Spend the next few minutes exploring the code and talking to your classmates – see if you can find and fix the bug. You'll know it's working if you can get a loss close to zero (say, less than 0.001)."
      ],
      "metadata": {
        "id": "7D25T9N5f3X_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNuYYN8VeC93"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MultilayerPerceptron(nn.Module):\n",
        "\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    # Call to the __init__ function of the super class\n",
        "    super(MultilayerPerceptron, self).__init__()\n",
        "\n",
        "    # Bookkeeping: Saving the initialization parameters\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    # Defining of our layers\n",
        "    self.linear = nn.Linear(self.input_size, self.hidden_size) # Ax + b ==> A.shape? b.shape? (go from size 5 to 3)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.linear2 = nn.Linear(self.hidden_size, self.input_size) # Ax + b ==> A.shape? b.shape? (go from size 3 to 5)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    linear = self.linear(x)\n",
        "    relu = self.relu(linear)\n",
        "    linear2 = self.linear2(relu)\n",
        "    return self.sigmoid(linear2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# Create the y data\n",
        "y = torch.ones(10, 5)\n",
        "\n",
        "# Add some noise to our goal y to generate our x\n",
        "# We want out model to predict our original data, albeit the noise\n",
        "x = y + torch.randn_like(y)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uoudhEreXxn",
        "outputId": "01bc5a26-9dfc-4e79-80ac-128544f0458b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 6.8993e-01, -9.2253e-02,  8.3026e-01,  9.7348e-01, -1.2449e+00],\n",
              "        [ 7.2571e-01,  1.0520e+00,  3.6243e+00,  2.2317e+00, -1.1718e-01],\n",
              "        [ 9.9008e-01, -6.3370e-01,  8.9606e-02,  1.2882e+00, -5.8181e-01],\n",
              "        [-6.5487e-01,  2.5902e-01, -4.2353e-01,  9.7073e-01,  2.1438e+00],\n",
              "        [-4.1000e-01, -4.8863e-01,  2.5953e+00,  1.0882e+00,  1.1918e+00],\n",
              "        [ 1.2210e+00,  1.1780e+00, -1.1729e+00,  8.8328e-01,  2.3593e+00],\n",
              "        [-2.5908e-01,  2.8247e-01, -1.8385e-01,  1.0181e+00,  1.6303e+00],\n",
              "        [ 1.6268e+00,  4.2486e-02,  2.7010e+00,  2.2068e+00,  1.3385e+00],\n",
              "        [-1.3439e+00,  1.5450e+00,  1.0289e+00, -9.2118e-01,  1.1189e-01],\n",
              "        [ 1.2770e+00,  7.5394e-04, -2.2318e-01,  8.4540e-01,  4.2415e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "model = MultilayerPerceptron(5, 3)\n",
        "adam = optim.Adam(model.parameters(), lr=1)\n",
        "# loss_function = nn.BCEWithLogitsLoss()\n",
        "loss_function = nn.BCELoss()\n",
        "\n",
        "# Set the number of epoch, which determines the number of training iterations\n",
        "n_epoch = 10\n",
        "\n",
        "for epoch in range(n_epoch):\n",
        "  # Set the gradients to 0\n",
        "  adam.zero_grad()\n",
        "\n",
        "  # Get the model predictions\n",
        "  y_pred = model(x)\n",
        "\n",
        "  # Get the loss\n",
        "  loss = loss_function(y_pred, y)\n",
        "\n",
        "  # Print stats\n",
        "  print(f\"Epoch {epoch}: traing loss: {loss}\")\n",
        "\n",
        "  # Compute the gradients\n",
        "  loss.backward()\n",
        "\n",
        "  # Take a step to optimize the weights\n",
        "  adam.step()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzBWROfceGo0",
        "outputId": "add1cf05-fe51-4a20-9bf2-678f75a3c9de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: traing loss: 0.6367056369781494\n",
            "Epoch 1: traing loss: 0.02494191564619541\n",
            "Epoch 2: traing loss: 5.452887762658065e-06\n",
            "Epoch 3: traing loss: 0.0\n",
            "Epoch 4: traing loss: 0.0\n",
            "Epoch 5: traing loss: 0.0\n",
            "Epoch 6: traing loss: 0.0\n",
            "Epoch 7: traing loss: 0.0\n",
            "Epoch 8: traing loss: 0.0\n",
            "Epoch 9: traing loss: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oVPJZouFlpZc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}